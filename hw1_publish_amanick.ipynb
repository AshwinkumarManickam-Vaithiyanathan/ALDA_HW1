{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-df76861e6b3d5417",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Homework 1\n",
    "\n",
    "In this homework you will be doing Decision Tree classification on the Breast Cancer dataset.\n",
    "We will also be using data preprocessing to test ways to improve the classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-445df16c3a9c5382",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1b92d511365b5816",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# you should be familiar with numpy, pandas and matplotlib from HW0\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# we're using the Breast Cancer dataset from sklearn.datasets\n",
    "from sklearn import datasets\n",
    "\n",
    "# we will also be using the PCA library from scikit learn for this exercise\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# we will use the StandardScaler method to z-score normalize our data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Remember you have to run this cell block before continuing!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cb4995075486ce2e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  target  \n",
       "0                  0.2654          0.4601                  0.11890       0  \n",
       "1                  0.1860          0.2750                  0.08902       0  \n",
       "2                  0.2430          0.3613                  0.08758       0  \n",
       "3                  0.2575          0.6638                  0.17300       0  \n",
       "4                  0.1625          0.2364                  0.07678       0  \n",
       "..                    ...             ...                      ...     ...  \n",
       "564                0.2216          0.2060                  0.07115       0  \n",
       "565                0.1628          0.2572                  0.06637       0  \n",
       "566                0.1418          0.2218                  0.07820       0  \n",
       "567                0.2650          0.4087                  0.12400       0  \n",
       "568                0.0000          0.2871                  0.07039       1  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "breast_cancer_sk = datasets.load_breast_cancer(as_frame=True)\n",
    "breast_cancer = pd.DataFrame(breast_cancer_sk.data, columns = breast_cancer_sk.feature_names)\n",
    "breast_cancer[\"target\"] = breast_cancer_sk.target\n",
    "breast_cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9c30171bad3bafef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1. Random Sampling [Anurata Hridi]\n",
    "\n",
    "In this following exercise, you will be writting code to implement random sampling without replacement from scratch. No additional libraries are allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random is the only additional library you can use for this problem\n",
    "import random\n",
    "\n",
    "def random_sampling(data, n):\n",
    "    \"\"\"\n",
    "    Input: data: the pandas dataframe to sample.\n",
    "           n: the number of samples (rows) to take\n",
    "    Output: The randomly sampled dataset (without replacement) as a pd.Dataframe.\n",
    "    Hint: You should look up the random.shuffle function\n",
    "    \"\"\"\n",
    "    indexes = data.index.to_numpy()\n",
    "    random.shuffle(indexes)\n",
    "    indexes = indexes[:n]\n",
    "    return data.filter(items = indexes, axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function\n",
    "sample = random_sampling(breast_cancer,30)                                                                                                                                                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Look at the distribution of the species (target attribute)\n",
    "# How evenly are the species distributed with random sampling?\n",
    "plt.hist(sample[\"target\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try running it again - are the results the same?\n",
    "# No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that the correct number of rows were samples\n",
    "np.testing.assert_equal(sample.shape,(30, 31))\n",
    "# Assert sampling was done without replacement\n",
    "assert sample.drop_duplicates().shape[0] == 30\n",
    "# Assert that the first row is present in the original dataframe\n",
    "assert any([(breast_cancer.iloc[i,:] == sample.iloc[0,:]).all() for i in breast_cancer.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d63a83e3a1c6f263",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2. Stratified sampling [Anurata Hridi]\n",
    "\n",
    "In this part, you will be writing code to do stratified sampling. Create a stratrified sample of the `breast_cancer` dataset, with 40 objects, that has an equal number of each **target** value (0 and 1).\n",
    "\n",
    "**Store it in the variable `stratified_breast_cancer`**.\n",
    "\n",
    "**Hint**: You should read about the [split-apply-combine](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html) coding pattern in Pandas before starting this problem! In particular pay attention to the following:\n",
    "* [Splitting an object into groups](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#splitting-an-object-into-groups)\n",
    "* [Transformation](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#transformation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_sampling(data, n, attr):\n",
    "    \"\"\"\n",
    "    Input: data: the dataset to sample\n",
    "           n: the number of instances sampled from each value of the given attribute\n",
    "           attr: the attribute to stratify on\n",
    "    Output: The sampled dataset in pd.Dataframe format\n",
    "    \n",
    "    Allowed functions: df.groupby, df.apply, df.sample\n",
    "    Hint: See the link in the function description above.\n",
    "    \"\"\"\n",
    "    grouped_data = data.groupby(attr, as_index = False).apply(lambda x: random_sampling(x, n), include_groups = True)\n",
    "\n",
    "    return grouped_data\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "stratified_sampling.<locals>.<lambda>() got an unexpected keyword argument 'include_groups'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1414\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1413\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1414\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_apply_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selected_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   1416\u001b[0m     \u001b[38;5;66;03m# gh-20949\u001b[39;00m\n\u001b[0;32m   1417\u001b[0m     \u001b[38;5;66;03m# try again, with .apply acting as a filtering\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1421\u001b[0m     \u001b[38;5;66;03m# fails on *some* columns, e.g. a numeric operation\u001b[39;00m\n\u001b[0;32m   1422\u001b[0m     \u001b[38;5;66;03m# on a string grouper column\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1455\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[1;34m(self, f, data, not_indexed_same)\u001b[0m\n\u001b[0;32m   1436\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1437\u001b[0m \u001b[38;5;124;03mApply function f in python space\u001b[39;00m\n\u001b[0;32m   1438\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1453\u001b[0m \u001b[38;5;124;03m    data after applying f\u001b[39;00m\n\u001b[0;32m   1454\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1455\u001b[0m values, mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrouper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:761\u001b[0m, in \u001b[0;36mBaseGrouper.apply\u001b[1;34m(self, f, data, axis)\u001b[0m\n\u001b[0;32m    760\u001b[0m group_axes \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39maxes\n\u001b[1;32m--> 761\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1388\u001b[0m, in \u001b[0;36mGroupBy.apply.<locals>.f\u001b[1;34m(g)\u001b[0m\n\u001b[0;32m   1387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(g, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: stratified_sampling.<locals>.<lambda>() got an unexpected keyword argument 'include_groups'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Test your function!\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m stratified_breast_cancer \u001b[38;5;241m=\u001b[39m \u001b[43mstratified_sampling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbreast_cancer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mstratified_sampling\u001b[1;34m(data, n, attr)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstratified_sampling\u001b[39m(data, n, attr):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    Input: data: the dataset to sample\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m           n: the number of instances sampled from each value of the given attribute\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    Hint: See the link in the function description above.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     grouped_data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_sampling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_groups\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grouped_data\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1425\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1415\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   1416\u001b[0m         \u001b[38;5;66;03m# gh-20949\u001b[39;00m\n\u001b[0;32m   1417\u001b[0m         \u001b[38;5;66;03m# try again, with .apply acting as a filtering\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1421\u001b[0m         \u001b[38;5;66;03m# fails on *some* columns, e.g. a numeric operation\u001b[39;00m\n\u001b[0;32m   1422\u001b[0m         \u001b[38;5;66;03m# on a string grouper column\u001b[39;00m\n\u001b[0;32m   1424\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group_selection_context():\n\u001b[1;32m-> 1425\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_apply_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selected_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1455\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[1;34m(self, f, data, not_indexed_same)\u001b[0m\n\u001b[0;32m   1429\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1430\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_python_apply_general\u001b[39m(\n\u001b[0;32m   1431\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1434\u001b[0m     not_indexed_same: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1435\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   1436\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1437\u001b[0m \u001b[38;5;124;03m    Apply function f in python space\u001b[39;00m\n\u001b[0;32m   1438\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1453\u001b[0m \u001b[38;5;124;03m        data after applying f\u001b[39;00m\n\u001b[0;32m   1454\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1455\u001b[0m     values, mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrouper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1458\u001b[0m         not_indexed_same \u001b[38;5;241m=\u001b[39m mutated \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmutated\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:761\u001b[0m, in \u001b[0;36mBaseGrouper.apply\u001b[1;34m(self, f, data, axis)\u001b[0m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[0;32m    760\u001b[0m group_axes \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39maxes\n\u001b[1;32m--> 761\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[0;32m    763\u001b[0m     mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1388\u001b[0m, in \u001b[0;36mGroupBy.apply.<locals>.f\u001b[1;34m(g)\u001b[0m\n\u001b[0;32m   1385\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m   1386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(g):\n\u001b[0;32m   1387\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1388\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(g, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: stratified_sampling.<locals>.<lambda>() got an unexpected keyword argument 'include_groups'"
     ]
    }
   ],
   "source": [
    "# Test your function!\n",
    "stratified_breast_cancer = stratified_sampling(breast_cancer, 10, 'target')\n",
    "\n",
    "# View your output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAXxUlEQVR4nO3de5DVdf348dcCsqCzIGpcNldF0lDAK8og+lWLifFC+k/qSAxRieWaKTMqpEiIsug4DpMRJKXQDIrWiDlKmFHEqHjj0lgqSpBSBuakLGIuyH5+f3zH/f5W8LJ0zgvO+njMfP7Yz3nv+bx4u8N5ei5sVVEURQAAJOmwpwcAAD5bxAcAkEp8AACpxAcAkEp8AACpxAcAkEp8AACpxAcAkKrTnh7gw5qbm+P111+PmpqaqKqq2tPjAACfQlEUsWXLlqitrY0OHT7+uY29Lj5ef/31qKur29NjAAC7YcOGDXHwwQd/7Jq9Lj5qamoi4n+H79at2x6eBgD4NBobG6Ourq7lcfzj7HXx8cFLLd26dRMfAFBhPs1bJrzhFABIJT4AgFTiAwBIJT4AgFTiAwBIJT4AgFTiAwBIJT4AgFTiAwBIJT4AgFRtjo9ly5bFyJEjo7a2NqqqquLBBx9sdXtRFHHDDTdEnz59omvXrjF8+PB45ZVXSjUvAFDh2hwfW7dujWOPPTZmzpy5y9tvvfXW+NGPfhSzZ8+Op59+Ovbbb78YMWJEvPfee//1sABA5WvzL5Y766yz4qyzztrlbUVRxIwZM+L666+P8847LyIifvGLX0SvXr3iwQcfjIsuuui/mxYAqHglfc/H+vXrY+PGjTF8+PCWc927d48hQ4bE8uXLd/k9TU1N0djY2OoAANqvNj/z8XE2btwYERG9evVqdb5Xr14tt31YQ0NDTJkypZRjfKzDJjySdq1S+dv0c/b0CAB8BI8rbbfHP+0yceLE2Lx5c8uxYcOGPT0SAFBGJY2P3r17R0TEpk2bWp3ftGlTy20fVl1dHd26dWt1AADtV0njo2/fvtG7d+9YsmRJy7nGxsZ4+umnY+jQoaW8FABQodr8no933nkn1q5d2/L1+vXrY/Xq1XHAAQfEIYccEldeeWXcdNNNccQRR0Tfvn1j0qRJUVtbG+eff34p5wYAKlSb4+O5556LM888s+Xr8ePHR0TEmDFjYu7cuXHNNdfE1q1bY9y4cfH222/HqaeeGosXL44uXbqUbmoAoGK1OT7OOOOMKIriI2+vqqqKG2+8MW688cb/ajAAoH3a4592AQA+W8QHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJCq5PGxY8eOmDRpUvTt2ze6du0a/fr1i6lTp0ZRFKW+FABQgTqV+g5vueWWmDVrVsybNy8GDBgQzz33XIwdOza6d+8eV1xxRakvBwBUmJLHx5NPPhnnnXdenHPOORERcdhhh8W9994bzzzzTKkvBQBUoJK/7HLKKafEkiVL4uWXX46IiD/96U/x+OOPx1lnnbXL9U1NTdHY2NjqAADar5I/8zFhwoRobGyM/v37R8eOHWPHjh1x8803x6hRo3a5vqGhIaZMmVLqMQCAvVTJn/m4//77Y/78+XHPPffEypUrY968eXHbbbfFvHnzdrl+4sSJsXnz5pZjw4YNpR4JANiLlPyZj6uvvjomTJgQF110UUREDBo0KF599dVoaGiIMWPG7LS+uro6qqurSz0GALCXKvkzH++++2506ND6bjt27BjNzc2lvhQAUIFK/szHyJEj4+abb45DDjkkBgwYEKtWrYrbb789vvnNb5b6UgBABSp5fNxxxx0xadKkuOyyy+KNN96I2trauPTSS+OGG24o9aUAgApU8vioqamJGTNmxIwZM0p91wBAO+B3uwAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcQHAJBKfAAAqcoSH//4xz/i61//ehx44IHRtWvXGDRoUDz33HPluBQAUGE6lfoO33rrrRg2bFiceeaZ8Zvf/CY+97nPxSuvvBI9evQo9aUAgApU8vi45ZZboq6uLu6+++6Wc3379i31ZQCAClXyl10eeuihGDx4cHzta1+Lnj17xvHHHx9z5sz5yPVNTU3R2NjY6gAA2q+Sx8e6deti1qxZccQRR8Sjjz4a3/3ud+OKK66IefPm7XJ9Q0NDdO/eveWoq6sr9UgAwF6k5PHR3NwcJ5xwQkybNi2OP/74GDduXFxyySUxe/bsXa6fOHFibN68ueXYsGFDqUcCAPYiJY+PPn36xNFHH93q3FFHHRWvvfbaLtdXV1dHt27dWh0AQPtV8vgYNmxYrFmzptW5l19+OQ499NBSXwoAqEAlj4+rrroqnnrqqZg2bVqsXbs27rnnnrjzzjujvr6+1JcCACpQyePjpJNOioULF8a9994bAwcOjKlTp8aMGTNi1KhRpb4UAFCBSv7vfEREnHvuuXHuueeW464BgArnd7sAAKnEBwCQSnwAAKnEBwCQSnwAAKnEBwCQSnwAAKnEBwCQSnwAAKnEBwCQSnwAAKnEBwCQSnwAAKnEBwCQSnwAAKnEBwCQSnwAAKnEBwCQSnwAAKnEBwCQSnwAAKnEBwCQSnwAAKnEBwCQSnwAAKnEBwCQSnwAAKnEBwCQSnwAAKnEBwCQSnwAAKnEBwCQSnwAAKnEBwCQSnwAAKnEBwCQSnwAAKnEBwCQSnwAAKnEBwCQSnwAAKnEBwCQSnwAAKnEBwCQSnwAAKnEBwCQSnwAAKnEBwCQSnwAAKnEBwCQSnwAAKnEBwCQSnwAAKnEBwCQSnwAAKnEBwCQSnwAAKnEBwCQSnwAAKnKHh/Tp0+PqqqquPLKK8t9KQCgApQ1Pp599tn46U9/Gsccc0w5LwMAVJCyxcc777wTo0aNijlz5kSPHj3KdRkAoMKULT7q6+vjnHPOieHDh3/suqampmhsbGx1AADtV6dy3OmCBQti5cqV8eyzz37i2oaGhpgyZUo5xgAA9kIlf+Zjw4YN8f3vfz/mz58fXbp0+cT1EydOjM2bN7ccGzZsKPVIAMBepOTPfKxYsSLeeOONOOGEE1rO7dixI5YtWxY//vGPo6mpKTp27NhyW3V1dVRXV5d6DABgL1Xy+Pjyl78czz//fKtzY8eOjf79+8e1117bKjwAgM+eksdHTU1NDBw4sNW5/fbbLw488MCdzgMAnz3+hVMAIFVZPu3yYUuXLs24DABQATzzAQCkEh8AQCrxAQCkEh8AQCrxAQCkEh8AQCrxAQCkEh8AQCrxAQCkEh8AQCrxAQCkEh8AQCrxAQCkEh8AQCrxAQCkEh8AQCrxAQCkEh8AQCrxAQCkEh8AQCrxAQCkEh8AQCrxAQCkEh8AQCrxAQCkEh8AQCrxAQCkEh8AQCrxAQCkEh8AQCrxAQCkEh8AQCrxAQCkEh8AQCrxAQCkEh8AQCrxAQCkEh8AQCrxAQCkEh8AQCrxAQCkEh8AQCrxAQCkEh8AQCrxAQCkEh8AQCrxAQCkEh8AQCrxAQCkEh8AQCrxAQCkEh8AQCrxAQCkEh8AQCrxAQCkEh8AQCrxAQCkEh8AQCrxAQCkEh8AQKqSx0dDQ0OcdNJJUVNTEz179ozzzz8/1qxZU+rLAAAVquTx8cc//jHq6+vjqaeeisceeyy2b98eX/nKV2Lr1q2lvhQAUIE6lfoOFy9e3OrruXPnRs+ePWPFihXxP//zP6W+HABQYUoeHx+2efPmiIg44IADdnl7U1NTNDU1tXzd2NhY7pEAgD2orG84bW5ujiuvvDKGDRsWAwcO3OWahoaG6N69e8tRV1dXzpEAgD2srPFRX18ff/7zn2PBggUfuWbixImxefPmlmPDhg3lHAkA2MPK9rLL5ZdfHg8//HAsW7YsDj744I9cV11dHdXV1eUaAwDYy5Q8PoqiiO9973uxcOHCWLp0afTt27fUlwAAKljJ46O+vj7uueee+PWvfx01NTWxcePGiIjo3r17dO3atdSXAwAqTMnf8zFr1qzYvHlznHHGGdGnT5+W47777iv1pQCAClSWl10AAD6K3+0CAKQSHwBAKvEBAKQSHwBAKvEBAKQSHwBAKvEBAKQSHwBAKvEBAKQSHwBAKvEBAKQSHwBAKvEBAKQSHwBAKvEBAKQSHwBAKvEBAKQSHwBAKvEBAKQSHwBAKvEBAKQSHwBAKvEBAKQSHwBAKvEBAKQSHwBAKvEBAKQSHwBAKvEBAKQSHwBAKvEBAKQSHwBAKvEBAKQSHwBAKvEBAKQSHwBAKvEBAKQSHwBAKvEBAKQSHwBAKvEBAKQSHwBAKvEBAKQSHwBAKvEBAKQSHwBAKvEBAKQSHwBAKvEBAKQSHwBAKvEBAKQSHwBAKvEBAKQSHwBAKvEBAKQSHwBAKvEBAKQSHwBAKvEBAKQqW3zMnDkzDjvssOjSpUsMGTIknnnmmXJdCgCoIGWJj/vuuy/Gjx8fkydPjpUrV8axxx4bI0aMiDfeeKMclwMAKkhZ4uP222+PSy65JMaOHRtHH310zJ49O/bdd9+46667ynE5AKCCdCr1HW7bti1WrFgREydObDnXoUOHGD58eCxfvnyn9U1NTdHU1NTy9ebNmyMiorGxsdSjRUREc9O7ZbnfcirXXgDw3/O40vo+i6L4xLUlj48333wzduzYEb169Wp1vlevXvHSSy/ttL6hoSGmTJmy0/m6urpSj1axus/Y0xMA0J6U83Fly5Yt0b17949dU/L4aKuJEyfG+PHjW75ubm6Of//733HggQdGVVVVSa/V2NgYdXV1sWHDhujWrVtJ75v/Y59z2Occ9jmPvc5Rrn0uiiK2bNkStbW1n7i25PFx0EEHRceOHWPTpk2tzm/atCl69+690/rq6uqorq5udW7//fcv9VitdOvWzQ92Avucwz7nsM957HWOcuzzJz3j8YGSv+G0c+fOceKJJ8aSJUtazjU3N8eSJUti6NChpb4cAFBhyvKyy/jx42PMmDExePDgOPnkk2PGjBmxdevWGDt2bDkuBwBUkLLEx4UXXhj/+te/4oYbboiNGzfGcccdF4sXL97pTajZqqurY/LkyTu9zENp2ecc9jmHfc5jr3PsDftcVXyaz8QAAJSI3+0CAKQSHwBAKvEBAKQSHwBAqnYXHzNnzozDDjssunTpEkOGDIlnnnnmY9f/8pe/jP79+0eXLl1i0KBBsWjRoqRJK1tb9nnOnDlx2mmnRY8ePaJHjx4xfPjwT/zvwv9q68/zBxYsWBBVVVVx/vnnl3fAdqKt+/z2229HfX199OnTJ6qrq+PII4/0d8en0NZ9njFjRnzxi1+Mrl27Rl1dXVx11VXx3nvvJU1bmZYtWxYjR46M2traqKqqigcffPATv2fp0qVxwgknRHV1dXzhC1+IuXPnln3OKNqRBQsWFJ07dy7uuuuu4i9/+UtxySWXFPvvv3+xadOmXa5/4oknio4dOxa33npr8cILLxTXX399sc8++xTPP/988uSVpa37fPHFFxczZ84sVq1aVbz44ovFN77xjaJ79+7F3//+9+TJK0tb9/kD69evLz7/+c8Xp512WnHeeeflDFvB2rrPTU1NxeDBg4uzzz67ePzxx4v169cXS5cuLVavXp08eWVp6z7Pnz+/qK6uLubPn1+sX7++ePTRR4s+ffoUV111VfLklWXRokXFddddVzzwwANFRBQLFy782PXr1q0r9t1332L8+PHFCy+8UNxxxx1Fx44di8WLF5d1znYVHyeffHJRX1/f8vWOHTuK2traoqGhYZfrL7jgguKcc85pdW7IkCHFpZdeWtY5K11b9/nD3n///aKmpqaYN29euUZsF3Znn99///3ilFNOKX72s58VY8aMER+fQlv3edasWcXhhx9ebNu2LWvEdqGt+1xfX1986UtfanVu/PjxxbBhw8o6Z3vyaeLjmmuuKQYMGNDq3IUXXliMGDGijJMVRbt52WXbtm2xYsWKGD58eMu5Dh06xPDhw2P58uW7/J7ly5e3Wh8RMWLEiI9cz+7t84e9++67sX379jjggAPKNWbF2919vvHGG6Nnz57xrW99K2PMirc7+/zQQw/F0KFDo76+Pnr16hUDBw6MadOmxY4dO7LGrji7s8+nnHJKrFixouWlmXXr1sWiRYvi7LPPTpn5s2JPPQ7u8d9qWypvvvlm7NixY6d/RbVXr17x0ksv7fJ7Nm7cuMv1GzduLNuclW539vnDrr322qitrd3pB57/szv7/Pjjj8fPf/7zWL16dcKE7cPu7PO6devi97//fYwaNSoWLVoUa9eujcsuuyy2b98ekydPzhi74uzOPl988cXx5ptvxqmnnhpFUcT7778f3/nOd+IHP/hBxsifGR/1ONjY2Bj/+c9/omvXrmW5brt55oPKMH369FiwYEEsXLgwunTpsqfHaTe2bNkSo0ePjjlz5sRBBx20p8dp15qbm6Nnz55x5513xoknnhgXXnhhXHfddTF79uw9PVq7snTp0pg2bVr85Cc/iZUrV8YDDzwQjzzySEydOnVPj0YJtJtnPg466KDo2LFjbNq0qdX5TZs2Re/evXf5Pb17927TenZvnz9w2223xfTp0+N3v/tdHHPMMeUcs+K1dZ//+te/xt/+9rcYOXJky7nm5uaIiOjUqVOsWbMm+vXrV96hK9Du/Dz36dMn9tlnn+jYsWPLuaOOOio2btwY27Zti86dO5d15kq0O/s8adKkGD16dHz729+OiIhBgwbF1q1bY9y4cXHddddFhw7+37kUPupxsFu3bmV71iOiHT3z0blz5zjxxBNjyZIlLeeam5tjyZIlMXTo0F1+z9ChQ1utj4h47LHHPnI9u7fPERG33nprTJ06NRYvXhyDBw/OGLWitXWf+/fvH88//3ysXr265fjqV78aZ555ZqxevTrq6uoyx68Yu/PzPGzYsFi7dm1L3EVEvPzyy9GnTx/h8RF2Z5/ffffdnQLjg+Ar/Eqyktljj4NlfTtrsgULFhTV1dXF3LlzixdeeKEYN25csf/++xcbN24siqIoRo8eXUyYMKFl/RNPPFF06tSpuO2224oXX3yxmDx5so/afgpt3efp06cXnTt3Ln71q18V//znP1uOLVu27Kk/QkVo6z5/mE+7fDpt3efXXnutqKmpKS6//PJizZo1xcMPP1z07NmzuOmmm/bUH6EitHWfJ0+eXNTU1BT33ntvsW7duuK3v/1t0a9fv+KCCy7YU3+EirBly5Zi1apVxapVq4qIKG6//fZi1apVxauvvloURVFMmDChGD16dMv6Dz5qe/XVVxcvvvhiMXPmTB+13R133HFHccghhxSdO3cuTj755OKpp55que30008vxowZ02r9/fffXxx55JFF586diwEDBhSPPPJI8sSVqS37fOihhxYRsdMxefLk/MErTFt/nv9/4uPTa+s+P/nkk8WQIUOK6urq4vDDDy9uvvnm4v3330+euvK0ZZ+3b99e/PCHPyz69etXdOnSpairqysuu+yy4q233sofvIL84Q9/2OXftx/s7ZgxY4rTTz99p+857rjjis6dOxeHH354cffdd5d9zqqi8PwVAJCn3bznAwCoDOIDAEglPgCAVOIDAEglPgCAVOIDAEglPgCAVOIDAEglPgCAVOIDAEglPgCAVOIDAEj1/wCxatJ6QbKV0gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# look at distribution of target values, they supposed to be equally sampled.\n",
    "plt.hist(stratified_breast_cancer[\"target\"])\n",
    "plt.show()\n",
    "assert(stratified_breast_cancer.shape[0] == 20)\n",
    "assert(sum(stratified_breast_cancer.target == 0) == 10)\n",
    "assert(sum(stratified_breast_cancer.target == 1) == 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-95e3fb2c7191077b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3. Decision Trees [Vodelina Samatova]\n",
    "\n",
    "Now we are going to classify the malignant versus benign cases (the zeros versus the ones) with Decision Trees.\n",
    "\n",
    "You can perform classification using a DecisionTree in python using the scikit-learn library. \n",
    "\n",
    "Take a look at the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) to get a clear understanding of all function arguments.\n",
    "\n",
    "Given below is a simple toy example for you to learn how to use the DecisionTreeClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-86cae1f3ae8215f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  target  \n",
       "0                  0.2654          0.4601                  0.11890       0  \n",
       "1                  0.1860          0.2750                  0.08902       0  \n",
       "2                  0.2430          0.3613                  0.08758       0  \n",
       "3                  0.2575          0.6638                  0.17300       0  \n",
       "4                  0.1625          0.2364                  0.07678       0  \n",
       "..                    ...             ...                      ...     ...  \n",
       "564                0.2216          0.2060                  0.07115       0  \n",
       "565                0.1628          0.2572                  0.06637       0  \n",
       "566                0.1418          0.2218                  0.07820       0  \n",
       "567                0.2650          0.4087                  0.12400       0  \n",
       "568                0.0000          0.2871                  0.07039       1  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "# we will use the MinMaxScaler method to scale our data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y: target\n",
    "y = breast_cancer[\"target\"]\n",
    "# Check the shape of y\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X: predictors\n",
    "# Let's drop the column for the target variable\n",
    "X = breast_cancer.drop(columns=[\"target\"])\n",
    "# Check the shape of X\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-08e1947248aafe00",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the first step, we will split our data into train and test subsets. To get more insight into the function we are using here (*train_test_split()*), take a look at [this tutorial.](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "* `X_train` is the features (independent variables) of the training dataset.\n",
    "* `y_train` is the target (dependent) variable of the training dataset.\n",
    "* `X_test` is the features (independent variables) of the test dataset.\n",
    "* `y_test` is the target (dependent) variable of the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(X, y, \n",
    "                     test_size=0.2, \n",
    "                     validate_size=0.2, \n",
    "                     random_state=0):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, stratify=y, test_size=test_size, random_state = random_state)\n",
    "\n",
    "    # We need to calculate a new split size (the proportion of validation to the remaining)\n",
    "    \n",
    "    # let's assume we had 100 samples and we don't do this\n",
    "    # then the split will be 20 + (20% of 80) + (80% of 80). \n",
    "    # But we want 20 + 20 + 60\n",
    "    new_validate_size = validate_size / (1 - test_size)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, stratify=y_train, \n",
    "        test_size=new_validate_size, \n",
    "        random_state = random_state)\n",
    "\n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0d6ef4b8b55ebc23",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Split data into training, validation, and testing data sets\n",
    "(X_train, X_test, X_val, y_train, y_test, y_val) = stratified_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (X_train and y_train): \t (341, 30)  \t (341,)\n",
      "Validation (X_val and y_val): \t\t (114, 30)  \t (114,)\n",
      "Testing (X_test and y_test): \t\t (114, 30)   \t (114,)\n"
     ]
    }
   ],
   "source": [
    "# Examine the split proportions \n",
    "\n",
    "print (\"Training (X_train and y_train): \\t\", X_train.shape, \" \\t\", y_train.shape)\n",
    "\n",
    "print (\"Validation (X_val and y_val): \\t\\t\", X_val.shape, \" \\t\", y_val.shape)\n",
    "\n",
    "print (\"Testing (X_test and y_test): \\t\\t\", X_test.shape, \"  \\t\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13.660</td>\n",
       "      <td>15.15</td>\n",
       "      <td>88.27</td>\n",
       "      <td>580.6</td>\n",
       "      <td>0.08268</td>\n",
       "      <td>0.07548</td>\n",
       "      <td>0.04249</td>\n",
       "      <td>0.02471</td>\n",
       "      <td>0.1792</td>\n",
       "      <td>0.05897</td>\n",
       "      <td>...</td>\n",
       "      <td>14.540</td>\n",
       "      <td>19.64</td>\n",
       "      <td>97.96</td>\n",
       "      <td>657.0</td>\n",
       "      <td>0.12750</td>\n",
       "      <td>0.3104</td>\n",
       "      <td>0.25690</td>\n",
       "      <td>0.10540</td>\n",
       "      <td>0.3387</td>\n",
       "      <td>0.09638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>9.295</td>\n",
       "      <td>13.90</td>\n",
       "      <td>59.96</td>\n",
       "      <td>257.8</td>\n",
       "      <td>0.13710</td>\n",
       "      <td>0.12250</td>\n",
       "      <td>0.03332</td>\n",
       "      <td>0.02421</td>\n",
       "      <td>0.2197</td>\n",
       "      <td>0.07696</td>\n",
       "      <td>...</td>\n",
       "      <td>10.570</td>\n",
       "      <td>17.84</td>\n",
       "      <td>67.84</td>\n",
       "      <td>326.6</td>\n",
       "      <td>0.18500</td>\n",
       "      <td>0.2097</td>\n",
       "      <td>0.09996</td>\n",
       "      <td>0.07262</td>\n",
       "      <td>0.3681</td>\n",
       "      <td>0.08982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>8.888</td>\n",
       "      <td>14.64</td>\n",
       "      <td>58.79</td>\n",
       "      <td>244.0</td>\n",
       "      <td>0.09783</td>\n",
       "      <td>0.15310</td>\n",
       "      <td>0.08606</td>\n",
       "      <td>0.02872</td>\n",
       "      <td>0.1902</td>\n",
       "      <td>0.08980</td>\n",
       "      <td>...</td>\n",
       "      <td>9.733</td>\n",
       "      <td>15.67</td>\n",
       "      <td>62.56</td>\n",
       "      <td>284.4</td>\n",
       "      <td>0.12070</td>\n",
       "      <td>0.2436</td>\n",
       "      <td>0.14340</td>\n",
       "      <td>0.04786</td>\n",
       "      <td>0.2254</td>\n",
       "      <td>0.10840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>14.270</td>\n",
       "      <td>22.55</td>\n",
       "      <td>93.77</td>\n",
       "      <td>629.8</td>\n",
       "      <td>0.10380</td>\n",
       "      <td>0.11540</td>\n",
       "      <td>0.14630</td>\n",
       "      <td>0.06139</td>\n",
       "      <td>0.1926</td>\n",
       "      <td>0.05982</td>\n",
       "      <td>...</td>\n",
       "      <td>15.290</td>\n",
       "      <td>34.27</td>\n",
       "      <td>104.30</td>\n",
       "      <td>728.3</td>\n",
       "      <td>0.13800</td>\n",
       "      <td>0.2733</td>\n",
       "      <td>0.42340</td>\n",
       "      <td>0.13620</td>\n",
       "      <td>0.2698</td>\n",
       "      <td>0.08351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>12.800</td>\n",
       "      <td>17.46</td>\n",
       "      <td>83.05</td>\n",
       "      <td>508.3</td>\n",
       "      <td>0.08044</td>\n",
       "      <td>0.08895</td>\n",
       "      <td>0.07390</td>\n",
       "      <td>0.04083</td>\n",
       "      <td>0.1574</td>\n",
       "      <td>0.05750</td>\n",
       "      <td>...</td>\n",
       "      <td>13.740</td>\n",
       "      <td>21.06</td>\n",
       "      <td>90.72</td>\n",
       "      <td>591.0</td>\n",
       "      <td>0.09534</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.19010</td>\n",
       "      <td>0.08296</td>\n",
       "      <td>0.1988</td>\n",
       "      <td>0.07053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>12.250</td>\n",
       "      <td>22.44</td>\n",
       "      <td>78.18</td>\n",
       "      <td>466.5</td>\n",
       "      <td>0.08192</td>\n",
       "      <td>0.05200</td>\n",
       "      <td>0.01714</td>\n",
       "      <td>0.01261</td>\n",
       "      <td>0.1544</td>\n",
       "      <td>0.05976</td>\n",
       "      <td>...</td>\n",
       "      <td>14.170</td>\n",
       "      <td>31.99</td>\n",
       "      <td>92.74</td>\n",
       "      <td>622.9</td>\n",
       "      <td>0.12560</td>\n",
       "      <td>0.1804</td>\n",
       "      <td>0.12300</td>\n",
       "      <td>0.06335</td>\n",
       "      <td>0.3100</td>\n",
       "      <td>0.08203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>13.240</td>\n",
       "      <td>20.13</td>\n",
       "      <td>86.87</td>\n",
       "      <td>542.9</td>\n",
       "      <td>0.08284</td>\n",
       "      <td>0.12230</td>\n",
       "      <td>0.10100</td>\n",
       "      <td>0.02833</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.06432</td>\n",
       "      <td>...</td>\n",
       "      <td>15.440</td>\n",
       "      <td>25.50</td>\n",
       "      <td>115.00</td>\n",
       "      <td>733.5</td>\n",
       "      <td>0.12010</td>\n",
       "      <td>0.5646</td>\n",
       "      <td>0.65560</td>\n",
       "      <td>0.13570</td>\n",
       "      <td>0.2845</td>\n",
       "      <td>0.12490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>12.470</td>\n",
       "      <td>18.60</td>\n",
       "      <td>81.09</td>\n",
       "      <td>481.9</td>\n",
       "      <td>0.09965</td>\n",
       "      <td>0.10580</td>\n",
       "      <td>0.08005</td>\n",
       "      <td>0.03821</td>\n",
       "      <td>0.1925</td>\n",
       "      <td>0.06373</td>\n",
       "      <td>...</td>\n",
       "      <td>14.970</td>\n",
       "      <td>24.64</td>\n",
       "      <td>96.05</td>\n",
       "      <td>677.9</td>\n",
       "      <td>0.14260</td>\n",
       "      <td>0.2378</td>\n",
       "      <td>0.26710</td>\n",
       "      <td>0.10150</td>\n",
       "      <td>0.3014</td>\n",
       "      <td>0.08750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>13.000</td>\n",
       "      <td>25.13</td>\n",
       "      <td>82.61</td>\n",
       "      <td>520.2</td>\n",
       "      <td>0.08369</td>\n",
       "      <td>0.05073</td>\n",
       "      <td>0.01206</td>\n",
       "      <td>0.01762</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.05449</td>\n",
       "      <td>...</td>\n",
       "      <td>14.340</td>\n",
       "      <td>31.88</td>\n",
       "      <td>91.06</td>\n",
       "      <td>628.5</td>\n",
       "      <td>0.12180</td>\n",
       "      <td>0.1093</td>\n",
       "      <td>0.04462</td>\n",
       "      <td>0.05921</td>\n",
       "      <td>0.2306</td>\n",
       "      <td>0.06291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>12.450</td>\n",
       "      <td>15.70</td>\n",
       "      <td>82.57</td>\n",
       "      <td>477.1</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.15780</td>\n",
       "      <td>0.08089</td>\n",
       "      <td>0.2087</td>\n",
       "      <td>0.07613</td>\n",
       "      <td>...</td>\n",
       "      <td>15.470</td>\n",
       "      <td>23.75</td>\n",
       "      <td>103.40</td>\n",
       "      <td>741.6</td>\n",
       "      <td>0.17910</td>\n",
       "      <td>0.5249</td>\n",
       "      <td>0.53550</td>\n",
       "      <td>0.17410</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>0.12440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>341 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "11        13.660         15.15           88.27      580.6          0.08268   \n",
       "341        9.295         13.90           59.96      257.8          0.13710   \n",
       "393        8.888         14.64           58.79      244.0          0.09783   \n",
       "157       14.270         22.55           93.77      629.8          0.10380   \n",
       "362       12.800         17.46           83.05      508.3          0.08044   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "558       12.250         22.44           78.18      466.5          0.08192   \n",
       "179       13.240         20.13           86.87      542.9          0.08284   \n",
       "524       12.470         18.60           81.09      481.9          0.09965   \n",
       "549       13.000         25.13           82.61      520.2          0.08369   \n",
       "261       12.450         15.70           82.57      477.1          0.12780   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "11            0.07548         0.04249              0.02471         0.1792   \n",
       "341           0.12250         0.03332              0.02421         0.2197   \n",
       "393           0.15310         0.08606              0.02872         0.1902   \n",
       "157           0.11540         0.14630              0.06139         0.1926   \n",
       "362           0.08895         0.07390              0.04083         0.1574   \n",
       "..                ...             ...                  ...            ...   \n",
       "558           0.05200         0.01714              0.01261         0.1544   \n",
       "179           0.12230         0.10100              0.02833         0.1601   \n",
       "524           0.10580         0.08005              0.03821         0.1925   \n",
       "549           0.05073         0.01206              0.01762         0.1667   \n",
       "261           0.17000         0.15780              0.08089         0.2087   \n",
       "\n",
       "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
       "11                  0.05897  ...        14.540          19.64   \n",
       "341                 0.07696  ...        10.570          17.84   \n",
       "393                 0.08980  ...         9.733          15.67   \n",
       "157                 0.05982  ...        15.290          34.27   \n",
       "362                 0.05750  ...        13.740          21.06   \n",
       "..                      ...  ...           ...            ...   \n",
       "558                 0.05976  ...        14.170          31.99   \n",
       "179                 0.06432  ...        15.440          25.50   \n",
       "524                 0.06373  ...        14.970          24.64   \n",
       "549                 0.05449  ...        14.340          31.88   \n",
       "261                 0.07613  ...        15.470          23.75   \n",
       "\n",
       "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
       "11             97.96       657.0           0.12750             0.3104   \n",
       "341            67.84       326.6           0.18500             0.2097   \n",
       "393            62.56       284.4           0.12070             0.2436   \n",
       "157           104.30       728.3           0.13800             0.2733   \n",
       "362            90.72       591.0           0.09534             0.1812   \n",
       "..               ...         ...               ...                ...   \n",
       "558            92.74       622.9           0.12560             0.1804   \n",
       "179           115.00       733.5           0.12010             0.5646   \n",
       "524            96.05       677.9           0.14260             0.2378   \n",
       "549            91.06       628.5           0.12180             0.1093   \n",
       "261           103.40       741.6           0.17910             0.5249   \n",
       "\n",
       "     worst concavity  worst concave points  worst symmetry  \\\n",
       "11           0.25690               0.10540          0.3387   \n",
       "341          0.09996               0.07262          0.3681   \n",
       "393          0.14340               0.04786          0.2254   \n",
       "157          0.42340               0.13620          0.2698   \n",
       "362          0.19010               0.08296          0.1988   \n",
       "..               ...                   ...             ...   \n",
       "558          0.12300               0.06335          0.3100   \n",
       "179          0.65560               0.13570          0.2845   \n",
       "524          0.26710               0.10150          0.3014   \n",
       "549          0.04462               0.05921          0.2306   \n",
       "261          0.53550               0.17410          0.3985   \n",
       "\n",
       "     worst fractal dimension  \n",
       "11                   0.09638  \n",
       "341                  0.08982  \n",
       "393                  0.10840  \n",
       "157                  0.08351  \n",
       "362                  0.07053  \n",
       "..                       ...  \n",
       "558                  0.08203  \n",
       "179                  0.12490  \n",
       "524                  0.08750  \n",
       "549                  0.06291  \n",
       "261                  0.12440  \n",
       "\n",
       "[341 rows x 30 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect X_train\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11     1\n",
       "341    1\n",
       "393    1\n",
       "157    0\n",
       "362    1\n",
       "      ..\n",
       "558    1\n",
       "179    1\n",
       "524    1\n",
       "549    1\n",
       "261    0\n",
       "Name: target, Length: 341, dtype: int32"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect y_train\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3ca4dd1d21003d81",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now implement a decision tree function below to do the following:\n",
    "\n",
    "1. Train a decision tree model with the **training datasets** created above (using **gini index** as the criteria and **random_state = 1234** in order to keep consistency).\n",
    "2. Use the model trained from step 1 to make predictions on the **test data** created above\n",
    "\n",
    "**Store the predictions in the variable `dtree_predictions`**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "dtree",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def dtree(X_train, y_train, X_test, criterion='gini', random_state=1234):\n",
    "\n",
    "    dtree = DecisionTreeClassifier(criterion='gini', random_state = random_state)\n",
    "    dtree = dtree.fit(X_train, y_train)\n",
    "    dtree_predictions = dtree.predict(X_test)\n",
    "\n",
    "    return dtree_predictions\n",
    "\n",
    "dtree_predictions = dtree(X_train, y_train, X_test, 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your result!\n",
    "dtree_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c87ab101a0fff1b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this step, you will look into the accuracy score of the decision tree predictions. In other words, you would compare the predictions of the decision tree with the actual test labels you have in the testing set. \n",
    "\n",
    "For more documentation, take a look at [this article.](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9035087719298246"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(dtree_predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "dtree-public",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Public tests for dtree\n",
    "np.testing.assert_almost_equal(\n",
    "    accuracy_score(dtree_predictions,\n",
    "                   y_test),\n",
    "    0.9035087719298246\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f2cfa4389baa6ce4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 4. Feature Selection [Vodelina Samatova]\n",
    "\n",
    "In this problem, you will see if feature selection can improve the accuracy of your classifier!\n",
    "\n",
    "We do feature selection to remove unnecessary features, and also to see which features are most useful for prediction. You could imagine predicting diabetes progression would be easier if we know which features are relevant for doing so.\n",
    "\n",
    "In the function below, you will implement a feature selection aglorithm, which selects the $k$ best features according to some measure. In this case we'll use the  [ANOVA F-value](https://www.statisticshowto.com/probability-and-statistics/f-statistic-value-test/), which is a measure of the linear relationship between each feature and the target variable (just like correlation).\n",
    "\n",
    "**Note**: Remember, we can't peek at the test data, even during feature selection (if we knew what features were useful on the test dataset, that would be unrealistic). Therefore, it is important that you fit the the feature selction function **ONLY** on the *training* set. One the feature selector is fit (i.e. figures out which features to keep), you can use it to transform both training and testing dataset (`X_train` and `X_test`), i.e. remove the unneeded features.\n",
    "\n",
    "**HINT**: Feature selection is performed using [SelectKBest function](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html) with [ANOVA F-value](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif) function.\n",
    "\n",
    "**HINT**: In python you can return multiple values, separated by commas, e.g. `return 1, 2, 3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "feature_selection",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "def feature_selection(X_train, y_train, X_test, k=7):\n",
    "    \"\"\"\n",
    "     Input:\n",
    "          x_train: A numpy array of shape (n_training_rows, n_attributes) where n_training_rows refers to \n",
    "          the number of rows in your training dataset and n_attributes refers to the number of attributes. \n",
    "          y_train: A numpy array of shape (n_training_rows, ) containing the class labels for each row in your \n",
    "          training dataset.\n",
    "          x_test: A numpy array of shape (n_testing_rows, n_attributes) where n_testing_rows refers to the number \n",
    "          of rows in your testing dataset and n_attributes refers to the number of attributes. \n",
    "          k: number of features to select.\n",
    "    Output:\n",
    "          fs: The fit feature selector\n",
    "          X_train_selected: The transformed training set, with features selected. \n",
    "                            The result should be a numpy array of shape (n_training_rows, k).\n",
    "          X_test_selected: The transformed testing set, with features selected.\n",
    "    \n",
    "    Allowed Libraries: sklearn\n",
    "    \"\"\"\n",
    "\n",
    "    # define feature selection\n",
    "    fs = SelectKBest(k = k)\n",
    "    fs.fit(X_train, y_train)\n",
    "\n",
    "    # apply feature selection\n",
    "    X_train_selected = fs.transform(X_train)\n",
    "    X_test_selected = fs.transform(X_test)\n",
    "\n",
    "    return fs, X_train_selected, X_test_selected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.88025770e+02 7.08854777e+01 4.18029021e+02 3.36880602e+02\n",
      " 5.01255702e+01 2.05381888e+02 3.95714153e+02 5.53613324e+02\n",
      " 4.34502810e+01 3.52685992e-02 1.84879323e+02 3.68284617e-01\n",
      " 1.77655322e+02 1.60615071e+02 4.51961967e-01 4.28794755e+01\n",
      " 3.85754122e+01 1.01944172e+02 1.46946004e-01 8.18170914e+00\n",
      " 4.79579810e+02 9.18694127e+01 4.95696897e+02 3.52940911e+02\n",
      " 6.96036727e+01 1.84940066e+02 2.81178021e+02 5.70465714e+02\n",
      " 6.35428353e+01 4.64644363e+01]\n",
      "(341, 7)\n",
      "(114, 7)\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "\n",
    "fs,X_train_selected,X_test_selected = feature_selection(X_train, y_train, X_test, 7)\n",
    "\n",
    "# Check the score for each individual feature (i.e. how important is each feature)\n",
    "print(fs.scores_)\n",
    "\n",
    "# Check whether acheived seven best attributes for both training and testing set\n",
    "print(X_train_selected.shape)\n",
    "print(X_test_selected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features are: \n",
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "Selected best feautures are: \n",
      "['mean radius' 'mean perimeter' 'mean concavity' 'mean concave points'\n",
      " 'worst radius' 'worst perimeter' 'worst concave points']\n"
     ]
    }
   ],
   "source": [
    "# Let's get the name of the selected features\n",
    "\n",
    "print(\"Original features are: \\n\" + str(X_train.columns.values))\n",
    "\n",
    "print(\"Selected best feautures are: \\n\" + str(X_train.columns.values[fs.get_support()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "feature_selection_public",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Public tests for feature_selection\n",
    "fs, selected_train, selected_test = feature_selection(X_train, y_train, X_test, 5)\n",
    "assert (selected_train.shape[1] == 5)\n",
    "assert (selected_test.shape[1] == 5)\n",
    "assert ('concavity error' not in X_train.columns.values[fs.get_support()])\n",
    "assert ('worst radius' in X_train.columns.values[fs.get_support()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-980493e5987405de2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 5. Feature Transformation [Sogolsadat Mansouri]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will combine feature selection with normalization into a *pipeline*, and test whether it improves our mode.\n",
    "\n",
    "In some cases, you would want to perform certain feature transformations such as z-score normalization.\n",
    "\n",
    "Take a look at the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to get a clear understanding of all function argumetns for z-score normalization.\n",
    "\n",
    "Given below is a simple example for you to learn how to use z-score normalization and combine it with other data processing procedures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use a toy dataset to demonstrate z-score normalization. \n",
    "# We are using the wine dataset from sklearn.datasets as our toy dataset.\n",
    "\n",
    "# First load the data\n",
    "toy_dataset = datasets.load_wine(as_frame=True).data\n",
    "\n",
    "# To emulate an actual model learning process, we split the data set into trainining and testing set\n",
    "toy_train, toy_test = train_test_split(toy_dataset, test_size = 0.33)\n",
    "\n",
    "# Here we use StandardScaler class from sklearn to normalize\n",
    "toy_norm_train = StandardScaler().fit(toy_train).transform(toy_train)\n",
    "\n",
    "# Notice here we use the training set to fit the StandardScaler, i.e. compute the mean and standard deviation,\n",
    "# and use it to normalize testing set, just like with feature selection, above.\n",
    "# This is because in real-world deployment case, you very likely won't know the parameter of unseen data distribution\n",
    "# (testing set). So a normal approach is to use distribution parameters estimated from training set to transform\n",
    "# unseen data.\n",
    "toy_norm_test = StandardScaler().fit(toy_train).transform(toy_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alcohol                          13.041429\n",
       "malic_acid                        2.270588\n",
       "ash                               2.354454\n",
       "alcalinity_of_ash                19.046218\n",
       "magnesium                        98.277311\n",
       "total_phenols                     2.321933\n",
       "flavanoids                        2.043866\n",
       "nonflavanoid_phenols              0.361008\n",
       "proanthocyanins                   1.582773\n",
       "color_intensity                   5.148908\n",
       "hue                               0.958655\n",
       "od280/od315_of_diluted_wines      2.598235\n",
       "proline                         764.092437\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the means are all different\n",
    "toy_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alcohol                           0.807531\n",
       "malic_acid                        1.060646\n",
       "ash                               0.278606\n",
       "alcalinity_of_ash                 3.198193\n",
       "magnesium                        12.449848\n",
       "total_phenols                     0.633837\n",
       "flavanoids                        0.992447\n",
       "nonflavanoid_phenols              0.121414\n",
       "proanthocyanins                   0.578393\n",
       "color_intensity                   2.326488\n",
       "hue                               0.233005\n",
       "od280/od315_of_diluted_wines      0.699809\n",
       "proline                         336.719644\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And the standard deviations vary\n",
    "toy_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7026529579335385e-15\n",
      "6.344131569286608e-17\n",
      "-2.2274432531870264e-15\n",
      "-4.79541709796076e-16\n"
     ]
    }
   ],
   "source": [
    "# Now column means are all near 0\n",
    "print(toy_norm_train[:,0].mean())\n",
    "print(toy_norm_train[:,1].mean())\n",
    "print(toy_norm_train[:,2].mean())\n",
    "print(toy_norm_train[:,3].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999999\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Now and standard deviations are near 1\n",
    "print(toy_norm_train[:,0].std())\n",
    "print(toy_norm_train[:,1].std())\n",
    "print(toy_norm_train[:,2].std())\n",
    "print(toy_norm_train[:,3].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2d0dab0264a912db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You will build a pipeline that perform both feature selection (using your earlier function) and z-score normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "normalize_feature_select",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def normalize_feature_select(X_train, y_train, X_test, k = 7):\n",
    "    \"\"\"\n",
    "    You will build a pipeline that perform the following steps:\n",
    "        1. z-score normalize the x_train and x_test using x_train.\n",
    "        2. perform feature selection.\n",
    "    \n",
    "    Your inputs and outputs are as shown below:\n",
    "    \n",
    "    Input:\n",
    "          x_train: A numpy array of shape (n_training_rows, n_attributes) where n_training_rows refers to \n",
    "              the number of rows in your training dataset and n_attributes refers to the number of attributes. \n",
    "          y_train: A numpy array of shape (n_training_rows, ) containing the class labels for each row in your \n",
    "              training dataset.\n",
    "          x_test: A numpy array of shape (n_test_rows, n_attributes) where n_test_rows refers to the number \n",
    "              of rows in your target dataset and n_attributes refers to the number of attributes. \n",
    "          k: number of features to select.\n",
    "    Output:\n",
    "          x_train_selected: A numpy array of shape (n_train_rows, n_selected_attributes) containing \n",
    "              z-score normalized data from x_train with selected features only. \n",
    "              n_selected_attributes is the number of selected features.\n",
    "          x_test_selected: A numpy array of shape (n_test_rows, n_selected_attributes) containing \n",
    "              z-score normalized data from x_test with selected features only. \n",
    "              n_selected_attributes is the number of selected features.\n",
    "          \n",
    "    Allowed Libraries: sklearn\n",
    "    \"\"\"\n",
    "\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_norm_train = scaler.transform(X_train)\n",
    "    X_norm_test = scaler.transform(X_test)\n",
    "    \n",
    "    _, X_train_selected, X_test_selected = feature_selection(X_norm_train, y_train, X_norm_test, k=k)\n",
    "    \n",
    "    return X_train_selected, X_test_selected\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.521813049162069e-16\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "#Test your code!\n",
    "X_norm_selected_train, X_norm_selected_test = normalize_feature_select(X_train, y_train, X_test, k = 7)\n",
    "\n",
    "# Mean of attribute 0 should be 0\n",
    "print(X_norm_selected_train[:,0].mean())\n",
    "# Standard deviation of attriute 0 should be 1\n",
    "print(X_norm_selected_train[:,0].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "normalize_feature_select-public",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Public tests for feature_selection\n",
    "selected_train, selected_test = normalize_feature_select(X_train, y_train, X_test, 5)\n",
    "assert (selected_train.shape[1] == 5)\n",
    "assert (selected_test.shape[1] == 5)\n",
    "for i in range(0, 5):\n",
    "    np.testing.assert_almost_equal(X_norm_selected_train[:,i].mean(), 0)\n",
    "    np.testing.assert_almost_equal(X_norm_selected_train[:,i].std(), 1)\n",
    "    # test set shouldn't be perfectly centered or scaled\n",
    "    assert X_norm_selected_test[:,i].mean() != 0\n",
    "    assert X_norm_selected_test[:,i].std() != 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b77e84f5052da690",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now is the moment of truth - did the decision tree improve our classifier? Let's compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9035087719298246"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, let's recreate the original decision tree.\n",
    "dtree_predictions = dtree(X_train, y_train, X_test, 'gini', 1234)\n",
    "accuracy_score(dtree_predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9122807017543859"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next the normalized, feature selected tree\n",
    "dtree_norm_selected_predictions = dtree(X_norm_selected_train, y_train, X_norm_selected_test, 'gini', 1234)\n",
    "accuracy_score(dtree_norm_selected_predictions, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0d8760508efb3d84",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Which features did better - the originals or the normalized, feature selected ones? How much of a difference did preprocessing make? Based on what you know about decision trees, z-score normalization and feature selection, why do you think this was the case? Answer below in 2-3 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "comparing-trees",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The accuracy score of the decision tree with original features is 90%. The accuracy of the normalized, feature selected decision tree is 91%. Therefore, the normalized and selected features did better. The accuracy is more in the case of feature selected and normalized decision tree because feature selection helped in removing irrelevant features and reducing noice. z-score normalization wouldn't have had a significant impact on the accuracy because decision trees are scale invariant. They don't need attribute transformation techniques to perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-980493e5987405de23",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 6. PCA [Sogolsadat Mansouri]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c1694bc4582d3eed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Examples\n",
    "\n",
    "You can perform PCA in python using the scikit-learn library. Take a look at the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) to get a clear understanding of all function arguments.\n",
    "\n",
    "Given below is a simple toy example for you to learn how to use PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-def830bb53f756ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# let's use a toy dataset to demonstrate PCA. We are using the wine dataset from sklearn.datasets as our toy dataset.\n",
    "# We will apply PCA on it and extract the first two principal components. \n",
    "# While there are ways to directly extract the principal components using the sklearn methods, for the purpose \n",
    "# of this exercise, we will first extract the eigen vectors and then calculate the principal components from these eigen vectors. \n",
    "\n",
    "# first, load and z-score normalize the data\n",
    "toy_dataset_sk = datasets.load_wine(as_frame=True)\n",
    "toy_dataset = pd.DataFrame(StandardScaler().fit_transform(toy_dataset_sk.data), columns = toy_dataset_sk.feature_names)\n",
    "\n",
    "# Display the dataset for your reference\n",
    "# note that you can use the display() method to display your pandas dataframe in Jupyter\n",
    "#display(toy_dataset)\n",
    "\n",
    "# apply PCA on the toy dataset and extract the eigen vectors of the first two principal components\n",
    "toy_pca = PCA(n_components = 2).fit(toy_dataset)\n",
    "toy_eigen_vectors = toy_pca.components_\n",
    "\n",
    "# now extract the first two principal components\n",
    "# Recall from class material how to do this.  \n",
    "# Take a look at matrix multiplication using numpy here: \n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.matmul.html\n",
    "toy_principal_components = np.matmul(toy_dataset.values , toy_eigen_vectors.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 2)\n",
      "(178, 13)\n"
     ]
    }
   ],
   "source": [
    "# Explore the outputs of the eigen vectors and principal components to gain a better understanding\n",
    "# Explore other outputs...\n",
    "print(toy_eigen_vectors.T.shape)\n",
    "print(toy_dataset.values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5ba4a2707304846b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 6.1: Extracting Eigen Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "principal_component_analysis",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Problem 1\n",
    "def principal_component_analysis(data, n_components):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "          data: Data frame with M numeric attributes.\n",
    "          n_components: the number of principal components to calculate \n",
    "                        (e.g. 2 for two principal components)\n",
    "    Output:\n",
    "          a numpy array of floating point numbers of shape (n_components, M), \n",
    "          containing the first n_components eigen vectors.\n",
    "            \n",
    "    \"\"\"\n",
    "    return PCA(n_components = n_components).fit(data).components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.99525297e-03, 2.14514374e-03, 3.45336870e-02, 5.14453612e-01,\n",
       "       3.88827136e-06, 4.03315839e-05, 8.18903314e-05, 4.75569636e-05])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "principal_component_analysis(X_train, 2)[0, :8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 30)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your function!\n",
    "# note that you can convert your pandas data frame into a numpy matrix by using the dataframe.values property\n",
    "eigen_vectors = principal_component_analysis(X_train, 2)\n",
    "# Each of the 2 eigen vectors should have a value/weight for each of the original 13 attributes\n",
    "eigen_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "principal_component_analysis-public",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert principal_component_analysis(X_train, 2).shape == (2, 30)\n",
    "np.testing.assert_almost_equal(\n",
    "    principal_component_analysis(X_train, 2)[0, :8], \n",
    "    np.array(\n",
    "        [4.99525297e-03, 2.14514374e-03, 3.45336870e-02, 5.14453612e-01,\n",
    "       3.88827136e-06, 4.03315839e-05, 8.18903314e-05, 4.75569636e-05]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b8065cf6aaf5b93c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 6.2. Calculating Principal Components from Eigen Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "principal_component_calculation",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Problem 2\n",
    "def principal_component_calculation(data, component_weights):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "          data: A data frame with N rows and M numeric attributes (e.g. 178 x 13). \n",
    "          component_weights: a numpy array of shape (n_components, M) containing the weights (eigen vector) of \n",
    "              the first n_components principal components \n",
    "              (i.e. the output from running principal_component_analysis from problem 1)\n",
    "    Output:\n",
    "          a pandas dataframe of shape (N, n_components) ontaining the n_components principal components,\n",
    "          calculated for each of the N rows in data.\n",
    "    Hint: Can you solve this with matrix multiplication? Check out the np.matmul function.\n",
    "    \"\"\"\n",
    "    return np.matmul(data, component_weights.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Principal components are =               0           1\n",
      "11   868.291103 -164.411521\n",
      "341  418.093546  -55.437415\n",
      "393  374.966714  -64.673713\n",
      "157  955.426179 -169.519136\n",
      "362  775.242686 -134.497198\n",
      "..          ...         ...\n",
      "558  780.278195  -83.161657\n",
      "179  915.691687  -92.041880\n",
      "524  836.047126  -67.430863\n",
      "549  812.934184 -126.096073\n",
      "261  888.198708  -31.182813\n",
      "\n",
      "[341 rows x 2 columns] and their shape is (341, 2)\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "\n",
    "# note that eigen_vectors are the eigen vectors you calculated earlier using principal_component_analysis. \n",
    "# We're calculating it again here for ease of use\n",
    "eigen_vectors = principal_component_analysis(X_train, 2)\n",
    "principal_component_values = principal_component_calculation(X_train, eigen_vectors)\n",
    "print(f'Principal components are = {principal_component_values} and their shape is {principal_component_values.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "principal_component_calculation-public",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_eigen_vectors = principal_component_analysis(X_train, 2)\n",
    "test_pc = principal_component_calculation(X_train, test_eigen_vectors)\n",
    "assert test_pc.shape == (X_train.shape[0], 2)\n",
    "np.testing.assert_almost_equal(test_pc.iloc[0, :].values, [868.291104, -164.411521], decimal=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b1eb7d2a4006c6a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 6.3: Visualize your results\n",
    "\n",
    "You've gained familiarity with matplotlib in HW0. Now use matplotlib to\n",
    "\n",
    "a) generate a plot with the first principal component on x-axis, second principal component on y-axis. \n",
    "\n",
    "b) Assign color to each data point according to the target value. You can do this using the splitted y_train or y_test, depending on which set you are plotting.  \n",
    "\n",
    "***Hint 1:*** Take a look at the [plt.scatter](https://matplotlib.org/3.3.3/api/_as_gen/matplotlib.pyplot.scatter.html) function. Pay close attention to the 'c' variable. \n",
    "\n",
    "c) Name the x-axis as \"PC1\", y-axis as \"PC2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is an example of a scatter plot, using the bmi and age of participants\n",
    "plt.scatter(X_train.iloc[:, 0], X_train.iloc[:, 2], c=y_train)\n",
    "plt.xlabel(X_train.columns[0])\n",
    "plt.ylabel(X_train.columns[2])\n",
    "plt.show()\n",
    "\n",
    "# How well does this plot separate the 2 different classes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does this plot separate the 2 different classes?\n",
    "We are able to see the separation between the two classes clearly. Hence the plot separates the 2 differnt classes well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c684b0d6c8af3bab",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Problem 3 (see question above)\n",
    "\n",
    "# recalculating eigen vectors and principal components for ease of use\n",
    "eigen_vectors = principal_component_analysis(X_train, 2)\n",
    "principal_component_values = principal_component_calculation(X_train, eigen_vectors)\n",
    "plt.scatter(principal_component_values.iloc[:, 0], principal_component_values.iloc[:, 1], c=y_train)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the first 2 PCs separate the data better than the first 2 numeric attributes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the scatter plots of the first two PCs and the scatter plots of the first two numeric attributes, we see that the first two PCs do not separate the data better than the first two scatter plots. The principal components only reduce the dimensionality and capture overall variance. This does not mean they will result in a better class separation"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
